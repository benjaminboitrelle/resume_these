\documentclass[a4papper, 10pt]{article}

 \usepackage[utf8]{inputenc}
 \usepackage[T1]{fontenc}
 \usepackage[normalem]{ulem}
 \usepackage[french]{babel}
 \usepackage{graphicx}
 \usepackage{caption}
 \usepackage{subcaption}
 \usepackage{enumerate}
 \usepackage{array}
 \usepackage{amsmath,amssymb,mathrsfs}
 \usepackage{url}
 \usepackage{fullpage}
 
 \title{\textbf{Intelligent detection layers for advanced tracking in high-energy physics} \\ \large \vskip 1ex
        Résumé de la thèse de doctorat}
\author{Benjamin Boitrelle \\ 
        Sous la direction de : \\
        Jérôme Baudot : Directeur de thèse à l'Université de Strasbourg \\
        Ingrid Maria Gregor : Encadrante au laboratoire d'accueil au DESY de Hambourg}
 \date{}

 \begin{document}

    \maketitle

% Questions posées : Flavor-tagging ILC, rapport embranchement du Higgs
% Méthode : montrer analyse Higgs avec résultats rapport d'embranchement
%           discuter 
% Résultats

  \section{Contexte de la thèse}

  Le 4 juillet 2012 au CERN à Genève (Suisse), les collaborations ATLAS et CMS ont annoncé les premiers résultats d'analyse des données acquises grâce au plus grand accélérateur de particules du monde, le Large Hadron Collider (LHC). 
  Les deux expériences ont présenté la découverte de la signature d'une particule compatible au boson prédit par le mécanisme de la brisure de la symétrie électro-faible de Brout-Englert-Higgs, le boson de Higgs.
  Bien que l'augmentation de l'énergie de collision du LHC pourrait permettre une meilleure compréhension de cette nouvelle particule et de contraindre encore plus les limites du Modèle Standard, voire de découvrir des traces de physique au delà de cette théorie, la complexité des évènements générés limite l'accès à certains paramètres fondamentaux.
  %En effet, les protons sont des particules composites, c'est-à-dire constituées d'un assemblage de particules, dont leur structure cache les paramètres fondamentaux de la collision

  Un nouveau grand projet en physique des hautes énergies est à l'étude : l'International Linear Collider (ILC). 
  Ce collisionneur linéaire de 31 kilomètres de long permettra la collision d'électrons et de positrons dans une échelle d'énergie comprise entre 250 GeV et 500 GeV, voire 1 TeV si l'accelérateur est amélioré, pour des polarisations différentes.
  %En effet, les électrons et positrons sont des particules fondamentales qui permettent de contrôler précisément l'énergie de collision et de sélectionner les processus à étudier grâce à la polarisation de ces dites particules.
  Il devrait permettre de mieux caractériser les particules déjà connues, comme le boson de Higgs grâce à son couplage avec les fermions, mais aussi d'étudier la matière noire. 
  Pour cela la partie centrale du détecteur dédiée à la reconstruction du vertex doit avoir à la fois une excellente résolution spatiale et un budget de matière ne dépassant pas quelques millièmes de la longueur de radiation. 
  Ce sous-détecteur, appelé détecteur de vertex, doit être optimisé afin de permettre la trajectométrie dans un milieu hautement dense en particules et de différencier la trajectoire des quarks b et c.

  La collaboration PLUME, qui implique l'IPHC de Strasbourg, le DESY à Hambourg et l'université de Bristol, met en place les outils permettant de surmonter ces défis grâce à une conception innovante d'échelles de trajectométrie double face pixelisée, appelée PLUME\footnote{\textbf{P}ixelated \textbf{L}adder with \textbf{U}ltra low \textbf{M}aterial \textbf{E}mbeded}. 
  Ce type d'objet est équipé de six capteurs à pixels CMOS ultra fins, alignés l'un à côté de l'autre, sur chaque face d'un support mécanique très léger et tente d'atteindre un record au niveau du budget de matière en se rapprochant de 0.3\% de $X_0$.
  Pour chaque trajectoire, deux positions seront mesurées, une par face. 
  Elles permettront d'évaluer le point d'intersection de la particule avec le détecteur, mais aussi son mouvement et son origine. 
  Si les outils permettant cette double mesure sont maitrisés et optimisés, cela augmentera considérablement les capacités de trajectométrie de ce type d'instrument.

  \section{Étude de la désintégration du boson de Higgs}

  Afin de comprendre les paramètres du système de détection, j'ai démarré une analyse de physique concernant la désintégration du boson de Higgs en deux paires de quarks charmé et anti-charmé à une énergie de centre de masse de 350 GeV.
  Comme l'expérience n'a pas encore débuté, seul des échantillons issus de simulations Monte Carlo sont disponibles.


  L'état final qui m'a intéressé est celui où le Higgs et deux neutrinos sont produits, soit par Higgstrahlung, soit par fusion de bosons W.
  L'étude m'a d'abord permis de comprendre l'avantage de la polarisation des électrons et positrons sur le canal de physique que l'on souhaite étudier.
  Par exemple, la contribution la fusion de boson W est atténuée lorsque les électrons sont droits et les positrons gauches.
  Le signal étudié est noyé dans un bruit de fond généré par d'autres processus. 
  En sélectionnant différentes coupes, comme le nombre de leptons seuls, l'impulsion transverse visible, la masse visible ou encore l'angle entre les deux jets, il est possible d'améliorer la significance (ratio du signal sur la racine carré du signal et du bruit) de 5 à presque 50 après sept sélections différentes. 
  Bien que le bruit soit diminué d'un facteur de plus de 200, le signal intéressant a lui aussi été diminué d'un facteur 1.4.
  La proportion de quarks charmés identifiés à la suite de cette sélection est de ... alors que ... des b sont identifiés. 
  Une sélection par analyse multi-variable et un desgin adéquat du détecteur de vertex permettrait d'améliorer ces résultats. A PROUVER LOL

  \section{Préparation d'une campagne de tests sous faisceaux}

  Comme décrit en introduction, l'objectif de la collaboration PLUME est d'atteindre un budget de matière se rapprochant de 0.3\% de $X_0$ pour une résolution spatiale meilleure que 4 microns.
  La structure mécanique est validée grâce à l'utilisation de MIMOSA-26, des détecteurs monolithiques complexes qui ont une résolution spatiale de 3 microns.
  Le traitement des données est directement intégré dans les photocites qui collectent les charges. 
  Il permet de numériser directement le signal, grâce à des discriminateurs et de réduire la bande-passante de transmission des données par le biais d'un système de suppression de zéro (ne prend pas en compte les zéros envoyés par les pixels, qui ne représentent pas un signal physique intéressant).
  Cette méthode permet d'enregistrer les informations individuelles de plus de un million d'impacts/cm$^2$/s sur un capteur contenant plus de 500000 pixels sur une surface de 2 cm$^2$.

  Chaque module doit être validé en laboratoire afin de s'assurer que le processus de fabrication n'altère pas les capteurs utilisés.
  Après une inspection visuelle afin de vérifier l'alignement de chaque capteur l'un par rapport à l'autre et qu'aucun d'eux, ni aucune connexion n'est été endommagé pendant l'assemblage, chaque échelle est testée électriquement.
  La consommation des capteurs et le contrôle JTAG est vérifié, pour ensuite évaluer les seuils des comparateurs qui vont permettre de discriminer le signal du bruit.
  Il est important de déterminer leur point de fonctionnement optimal, leur bruit et leur offset. 
  Une analyse permet enfin de déterminer le taux de fantôme de chaque capteur et de vérifier qu'ils détectent correctement une source radioactive.
  
  Actuellement, différentes versions des échelles PLUME existent : celles dont le budget de matière est de 0.6 \% de X0 utilisant uniquement des pistes métallisées au cuivre; deux nouveaux prototypes, l'un utilisant des pistes métallisées en cuivre et l'autre en aluminium et dont les zones mortes de détection ont été réduites et dont la densité de la mousse mécanique a été diminuée de moitié, ont été produits.
  Bien que différentes versions existent, seul les modules atteignant un budget de matière de 0.6 \% de X0 ont été étudiés lors de deux campagnes en faisceau test, l'une réalisée par la collaboration en 2011 au CERN et l'autre que j'ai mené en avril 2016 au DESY.
  Les résultats de la première campagne a permis à la collaboration de mettre en avant les avantages d'une double mesure. 
  De ces résultats, je me suis intéressé à l'étude des déformations mécaniques de nos échelles et leur impact sur les résultats d'analyse.
  En effet, ce sont les premiers objets fabriqués avec des capteurs ultra fins (d'une épaisseur de seulement 50 $\mu$m) et ultra précis. 
  Les contraintes mécaniques induisent des déformations permanentes de quelques dizaines de micromètres de la surface qui ne peuvent être contrôlées lors de la construction.
  Apprendre à quantifier ces déformations et les prendre en compte pendant notre analyse est essentiel pour valider nos prototypes.
  En effet, lorsque l'échelle est inclinée dans une direction et que le faisceau ne la touche plus en incidence normale, la résolution spatiale se dégrade dans des proportions inattendues.
  Lors des l'analyse, les capteurs sont modélisés par une surface parfaitement plane.
  Or, la position de ces plans en trois dimensions est différente puisque ceux-ci peuvent être plus ou moins déformés.
  Ainsi comme il a été observé, la distribution du résidu devient plus importante lorsque l'angle d'incidence n'est plus normal à la surface du détecteur.
  Il faut donc prendre en compte cette déformation dans notre analyse afin de recalculer la position exacte de chaque pixel en 3 dimensions et l'extrapolation exacte sur le plan de la trajectoire. 
  Grâce à une première étude réalisée par un doctorant du groupe PICSEL et un article de la collaboration CMS sur l'alignement du trajectomètre\footnote{arXiv:0910.2505 [physics.ins-det]}, il m'a été possible de mettre en place un algorithme permettant de déterminer la forme de notre capteur à l'aide de polynômes de Legendre. 
  En prenant en compte l'angle d'incidence des particules, la résolution spatiale est améliorée. 
  Par exemple, l'analyse d'une acquisition où le module PLUME est incliné de 36\degre, a mis en évidence une déformation en corrélant le résidu (distance entre la position de la trace extrapolée et du pixel touché) à la position de l'impact sur la matrice de détection (figure \ref{deformed}), par rapport à une acquisition où le plan est en incidence normale (figure\ref{normal}).
  En ajustant la figure précédente par un polynôme de Legendre (figure \ref{legendre}), les coefficients obtenus permettent de paramétrer la surface du capteur et ainsi de minimiser le résidu (figure \ref{corrected}). 
  La déviation standard de les distribution des résidus, qui définit la résolution spatiale, passe de 11.84 $\mu m$ à 8.2 $\mu m$, sans prendre en compte la résolution du télescope qui n'est pas dans une configuration optimale.

  Nos échelles doivent avoir des performances similaires à basse énergie.
  C'est pourquoi, j'ai préparé et effectué une deuxième campagne de faisceau test avec des électrons de quelques GeV au DESY en avril 2016.
  Avant d'effectuer cette expérience, il m'a fallu m'assurer de l'intégration de notre détecteur au sein du système d'acquisition EUDAQ fournit par le DESY. 
  Une simulation estimant la résolution spatiale en fonction de différentes configurations de géométrie de télescope m'ont permis de concentrer sur un système comportant quatre plans de télescope.
  Comme la technologie des capteurs utilisés pour le télescope et PLUME sont les mêmes, le système d'acquisition a été simplifié. 
  Des mesures de plusieurs heures ont permis de vérifier la stabilité du système. 
  En même temps, un support rotatif a été imaginé afin de maintenir l'échelle à la position verticale.
  La rotation est effectuée grâce à un contrôleur et a permis pendant l'expérience de prendre des données pour des angles variant de 0 à 60 degrés. 
  L'analyse est en cours de route et devrait permette de confirmer l'avantage d'une mesure double face grâce à la reconstruction de mini-vecteurs (combinaison de deux hits sur chaque face de l'échelle) donnant une information sur la résolution angulaire et améliorant la résolution du détecteur, mais également l'analyse devrait permettre d'estimer et de comparer le budget de matière aux données théoriques.
   

  \section{Publications et conférences}

  Conférences :
  \begin{itemize}
    \item Beam Telescope and Test Beam, Janvier 2015, DESY - Hambourgm : présentation orale
    \item Linear collider Whistler, B.C, Canada, Novembre 2015 : présentation orale 

  \end{itemize}

  \section{Formations}
 
  \begin{itemize}
    \item  Linear Collider Physics School\footnote{https://indico.desy.de/conferenceDisplay.py?confId=7513} au DESY à Hambourg du 7 au 9 octobre 2013
    \item  7$^{th}$ Detector Workshop of the Terascale Alliance\footnote{https://indico.desy.de/conferenceDisplay.py?confId=9389} à Göttingen du 3 au 5 mars 2014
    \item  Introduction to Terascale 2014\footnote{https://indico.desy.de/conferenceDisplay.py?confId=9263} au DESY à Hambourg du 17 au 21 mars 2014
    \item  Linear Collider School 2014\footnote{https://indico.desy.de/conferenceDisplay.py?confId=9329} à Frauenchiemsee du 11 au 15 août 2014
    \item Cours d'allemand au DESY, septembre 2013 à février 2014 (3 heures par semaine)
    \item Cours d'allemand avec la PIER school (1 heure 30 par semaine)
  \end{itemize}

 \end{document}



